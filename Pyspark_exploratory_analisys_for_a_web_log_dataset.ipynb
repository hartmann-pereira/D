{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Pyspark exploratory analisys for a web log dataset",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hartmann-pereira/D/blob/main/Pyspark_exploratory_analisys_for_a_web_log_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZONWz-2526os"
      },
      "source": [
        "# Python Spark Example\n",
        "\n",
        "Notebook only for experimental use, built in pyspark.\n",
        "\n",
        "This notebook exemplifies the execution of a Spark program in Python.\n",
        "In this example, spark runs in standalone mode and reads data from the local filesystem, while in cluster mode data is read typically from HDFS dsitributed file system.\n",
        "\n",
        "Data came from a txt file with web log files.\n",
        "This was purely made to train myself to code in pyspark as this is a usefull skill to have, also wanted to show a bit of analisys that can be done with this tool. \n",
        "\n",
        "A lot of data printing would not be made if this was made with a bigger file, but for experimental purposes and because i knew how big the dataset is its fine.\n",
        "\n",
        "Spark documentation available at:\n",
        "https://spark.apache.org/docs/2.3.1/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEu8wkWB26o0"
      },
      "source": [
        "## Web log analisys example\n",
        "Firstly we are going to import all the dependencies necessary to run pyspark in a google colab environment. \n",
        "\n",
        "Brief index of what the notebook contains. More to come in a near future...\n",
        "\n",
        "+ We are going to manipulate a web log dataset.\n",
        "\n",
        "  + Return the list of source ip adresses\n",
        "  + Find log entries that report acces to a given url\n",
        "  + Create an inverted index url -> list of unique source ip adresses\n",
        "  + Count the number of unique ip adresses involved in the attack.\n",
        "  + and much more...!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pfoMiLo26o1"
      },
      "source": [
        "# instalation and imports of all the necessary dependencies, had to copy a lot from a google colab page\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "# configure the variables of the environment \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# enable pyspark to be imported \n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')\n",
        "\n",
        "import pyspark\n",
        "from operator import add as sum\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U07lIxwh6Y8_",
        "outputId": "88319554-e414-4dab-d040-d56361cf0b48"
      },
      "source": [
        "!wget -O web_log.txt https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-03 20:12:09--  https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6022:18::a27d:4212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/0r8902uj9yum7dg/web.log [following]\n",
            "--2021-11-03 20:12:10--  https://www.dropbox.com/s/raw/0r8902uj9yum7dg/web.log\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1ee7ec1446ddbc114ef780c747.dl.dropboxusercontent.com/cd/0/inline/BZTP7tMbvXw6x68G2QFEg3w9oiVse1uSM-ojqDz15nnZzG1q7WhAGCgQJ26nNFDqw1vlOnko9lGdMcDy9TwnBDg-AtLDSNPjsB01_22mbnJ7PXEF4ibRHS25iZv6Jo0Qn7V2pkgxnLirm7xJtxJgJgiA/file# [following]\n",
            "--2021-11-03 20:12:10--  https://uc1ee7ec1446ddbc114ef780c747.dl.dropboxusercontent.com/cd/0/inline/BZTP7tMbvXw6x68G2QFEg3w9oiVse1uSM-ojqDz15nnZzG1q7WhAGCgQJ26nNFDqw1vlOnko9lGdMcDy9TwnBDg-AtLDSNPjsB01_22mbnJ7PXEF4ibRHS25iZv6Jo0Qn7V2pkgxnLirm7xJtxJgJgiA/file\n",
            "Resolving uc1ee7ec1446ddbc114ef780c747.dl.dropboxusercontent.com (uc1ee7ec1446ddbc114ef780c747.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to uc1ee7ec1446ddbc114ef780c747.dl.dropboxusercontent.com (uc1ee7ec1446ddbc114ef780c747.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11758533 (11M) [text/plain]\n",
            "Saving to: ‘web_log.txt’\n",
            "\n",
            "web_log.txt         100%[===================>]  11.21M  8.65MB/s    in 1.3s    \n",
            "\n",
            "2021-11-03 20:12:12 (8.65 MB/s) - ‘web_log.txt’ saved [11758533/11758533]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHFrJRPw6gVe"
      },
      "source": [
        "sc = pyspark.SparkContext('local[*]') #initialized a spark context, remember than your code stops the spark context, so before running you have to run this code again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_jMpdHk5I-8"
      },
      "source": [
        "Exploratory analisys of web log data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nwi4zgJ6kaJ"
      },
      "source": [
        "try:\n",
        "  lines = sc.textFile('web_log.txt') # here we get the web log document into out spark context to manipulate as we want\n",
        "  non_empty_lines = lines.filter(lambda line: len(line) > 0) #data cleaning: getting an rdd with all our info\n",
        "  ips = non_empty_lines.map(lambda line:line.split(\" \")[1]) #data cleaning: our data has a lot of info but we want an rdd with only the ip adresses\n",
        "  distinct_ips = ips.distinct() # data cleaning: since it is a web log entry we have a lot of duplication, distinct ips are only one occurence per ip\n",
        "  for v in distinct_ips.collect():       # here we just want to output the distinct ips rdd\n",
        "    print(v) \n",
        "  sc.stop()\n",
        "except:\n",
        "  sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-lo7EZj26o3"
      },
      "source": [
        "try:\n",
        "  lines = sc.textFile('web_log.txt') # here we get the web log document into out spark context to manipulate as we want\n",
        "  non_empty_lines = lines.filter(lambda line: len(line) > 0) #data cleaning: getting an rdd with all our info\n",
        "  urls_accessed = non_empty_lines.map(lambda line: line.split(\" \")[4]) #we want to build an rdd with url info taken from the initial rdd\n",
        "  distinct_url_accessed = urls_accessed.distinct()  #again we want to create an rdd with only one occurence of the url\n",
        "  for x in distinct_url_accessed.collect():\n",
        "    print(x)\n",
        "  sc.stop()\n",
        "except:\n",
        "  sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkbsdShwCLdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2056b692-15af-45ef-b1e0-0304dbcc2303"
      },
      "source": [
        "try:\n",
        "  lines = sc.textFile('web_log.txt') # here we get the web log document into out spark context to manipulate as we want\n",
        "  non_empty_lines = lines.filter(lambda line: len(line) > 0) #data cleaning: getting an rdd with all our info\n",
        "  urls_accessed = non_empty_lines.map(lambda line: line.split(\" \")[4]) #we want to build an rdd with url info taken from the initial rdd\n",
        "  distinct_url_accessed = urls_accessed.distinct()  #again we want to create an rdd with only one occurence of the url\n",
        "  URL = '/codemove/TTCENCUFMH3C'  # we have a variable that holds a specific value we want to 'search' in our data file \n",
        "  ip_url = non_empty_lines.filter(lambda line: line.split(\" \")[4] == URL) # here we want to filter all the lines in our data file that do not hold our specific value to search\n",
        "  for y in ip_url.collect(): #print our list of values, we will see that many ip adresses made a conection with a specific url that we gave\n",
        "    print(y)\n",
        "  sc.stop()\n",
        "except:\n",
        "  sc.stop()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016-12-06T08:58:35.318+0000 37.139.9.11 404 GET /codemove/TTCENCUFMH3C 0.026  \n",
            "2016-12-06T08:58:35.389+0000 37.139.9.11 404 GET /codemove/TTCENCUFMH3C 0.057  \n",
            "2016-12-06T08:58:35.531+0000 37.139.9.11 404 GET /codemove/TTCENCUFMH3C 0.015  \n",
            "2016-12-06T08:58:35.815+0000 37.139.9.11 404 GET /codemove/TTCENCUFMH3C 0.039  \n",
            "2016-12-06T08:58:36.990+0000 37.139.9.11 404 GET /codemove/TTCENCUFMH3C 3.051  \n",
            "2016-12-06T08:58:37.786+0000 37.139.9.11 404 GET /codemove/TTCENCUFMH3C 41.512  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGQ1D-fdCSqT"
      },
      "source": [
        "try:\n",
        "  lines = sc.textFile('web_log.txt') # here we get the web log document into out spark context to manipulate as we want\n",
        "  non_empty_lines = lines.filter(lambda line: len(line) > 0) #data cleaning: getting an rdd with all our info\n",
        "  urls_accessed = non_empty_lines.map(lambda line: line.split(\" \")[4]) #we want to build an rdd with url info taken from the initial rdd\n",
        "  distinct_url_accessed = urls_accessed.distinct()  #again we want to create an rdd with only one occurence of the url\n",
        "  ips = non_empty_lines.map(lambda line:line.split(\" \")[1]) #data cleaning: our data has a lot of info but we want an rdd with only the ip adresses\n",
        "  distinct_ips = ips.distinct() # data cleaning: since it is a web log entry we have a lot of duplication, distinct ips are only one occurence per ip\n",
        "  \n",
        "  answer = non_empty_lines.map(lambda line: line.split()).map(lambda v: (v[4] , {v[1]}))  #i want to get an inverted index with url and ips that accessed it, i will first get the combination mapping url to ip adress\n",
        "  answer_final = answer.reduceByKey(lambda a,b: a | b)  #after getting previous rdd i will reduce by key having now distinct url and distinct ips that accessed it \n",
        "\n",
        "  URL = '/codemove/TTCENCUFMH3C'  # we have a variable that holds a specific value we want to 'search' in our data file \n",
        "  ip_url = non_empty_lines.filter(lambda line: line.split(\" \")[4] == URL) # here we want to filter all the lines in our data file that do not hold our specific value to search\n",
        "  for z,w in answer_final.collect(): #print our list of values, we will see that many ip adresses made a conection with a specific url that we gave\n",
        "    print(z,w)\n",
        "  sc.stop()\n",
        "except:\n",
        "  sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}