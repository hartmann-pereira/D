{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPdK7J8b2me/xYt4fI9ngGE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hartmann-pereira/D/blob/main/Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure stuff"
      ],
      "metadata": {
        "id": "GjSCY7gBvir4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell is setting up Pyspark in google-colab\n",
        "1. Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java.\n",
        "2. Next, we will download and unzip Apache Spark with Hadoop 2.7 to install it. [Spark version 3.2.0 (latest stable) and hadoop 2.7]\n",
        "3. After this we need to import os and set the ‘environment’ path.\n",
        "4. Then we need to install and import the ‘findspark’ library that will locate Spark on the system and import it as a regular library."
      ],
      "metadata": {
        "id": "NokTi0q88vHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqwQC0gayNzu",
        "outputId": "e1fdd856-9578-41ce-ac0d-447bf400f323"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 44 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 72.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=97580e17c4e2a08156744b24e3803751fd8ebad207c477046883ce9a41d6d5e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly import finspark as a regular library and initialize it"
      ],
      "metadata": {
        "id": "mFhnye4u9mMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "-yteBA0Rz5lu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the final step needed is to start a Spark session, importing SparkSession from library and we want to run it localy, name this Colab for simplicity, get or create, its gonna create because this is the first time we running this."
      ],
      "metadata": {
        "id": "eVfZHpmn9rl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "RAmZtsP78k7g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thats all done, let's get started with Pyspark and run a little simple project for this session"
      ],
      "metadata": {
        "id": "ex9tvOV0-B4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can download the file directly into Colab using the ‘wget’ command like this to get my file from github "
      ],
      "metadata": {
        "id": "KAb2VjOR-4Rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import my dataset into my workbook and into a spark df"
      ],
      "metadata": {
        "id": "hPOIItqxD7oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --continue https://raw.githubusercontent.com/GarvitArya/pyspark-demo/main/sample_books.json -O /tmp/sample_books.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myyqtl5f8p1W",
        "outputId": "14c9b65b-220d-4930-d3e7-230b7bef0e7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-08 16:57:33--  https://raw.githubusercontent.com/GarvitArya/pyspark-demo/main/sample_books.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1565 (1.5K) [text/plain]\n",
            "Saving to: ‘/tmp/sample_books.json’\n",
            "\n",
            "\r/tmp/sample_books.j   0%[                    ]       0  --.-KB/s               \r/tmp/sample_books.j 100%[===================>]   1.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-08 16:57:33 (19.9 MB/s) - ‘/tmp/sample_books.json’ saved [1565/1565]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.json(\"/tmp/sample_books.json\")"
      ],
      "metadata": {
        "id": "D-fLrKzz-XAE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXX1QRjG-vxh",
        "outputId": "6d227162-c3cd-4e8b-8538-eb2e4905cd60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- author: string (nullable = true)\n",
            " |-- edition: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- year_written: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check what our dataset looks like \n",
        "df.show(4,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up76j9YJ-2eC",
        "outputId": "5bb08984-453b-4f11-f788-8bc4864c82f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------+-----+----------------+------------+\n",
            "|author         |edition       |price|title           |year_written|\n",
            "+---------------+--------------+-----+----------------+------------+\n",
            "|Austen, Jane   |Penguin       |18.2 |Northanger Abbey|1814        |\n",
            "|Tolstoy, Leo   |Penguin       |12.7 |War and Peace   |1865        |\n",
            "|Tolstoy, Leo   |Penguin       |13.5 |Anna Karenina   |1875        |\n",
            "|Woolf, Virginia|Harcourt Brace|25.0 |Mrs. Dalloway   |1925        |\n",
            "+---------------+--------------+-----+----------------+------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysis"
      ],
      "metadata": {
        "id": "kepNqBVXEKSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#How big is this dataset?\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r07p6mHl_PvT",
        "outputId": "3bedc09d-7939-45d1-dfdd-e9b0272b76d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see a few columns of interest \n",
        "df.select(\"title\",\"price\",\"year_written\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvdljfNP_eXH",
        "outputId": "c4a2a80f-25ef-448c-a526-073d2153483b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----+------------+\n",
            "|           title|price|year_written|\n",
            "+----------------+-----+------------+\n",
            "|Northanger Abbey| 18.2|        1814|\n",
            "|   War and Peace| 12.7|        1865|\n",
            "|   Anna Karenina| 13.5|        1875|\n",
            "|   Mrs. Dalloway| 25.0|        1925|\n",
            "|       The Hours|12.35|        1999|\n",
            "+----------------+-----+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we could filter this dataset a bit to get some insight into some questions\n",
        "#Let's get the books that are written after 1995 and cost 10 bucks\n",
        "df_filtered = df.filter(\"year_written > 1995 AND price > 10 AND title IS NOT NULL\")\n",
        "df_filtered.show(59,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6tAOZhr_zU5",
        "outputId": "59ee6f49-dcd8-412f-b762-95810086882b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+-----+------------+------------+\n",
            "|author              |edition       |price|title       |year_written|\n",
            "+--------------------+--------------+-----+------------+------------+\n",
            "|Cunnningham, Michael|Harcourt Brace|12.35|The Hours   |1999        |\n",
            "|Rowling, J.K.       |Harcourt Brace|19.95|Harry Potter|2000        |\n",
            "+--------------------+--------------+-----+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Doing a bit more analysis, let's get all the books from previous dataset that contain Harry Potter in the title\n",
        "df_filtered.select(\"title\").filter(\"title LIKE 'Harry Potter'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT6xyR8oAq-c",
        "outputId": "5c250213-f708-4b47-e221-6925954396e8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|       title|\n",
            "+------------+\n",
            "|Harry Potter|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Previous cell only provided the title, lets get all other rows\n",
        "df_filtered.select(\"title\", \"edition\",\"price\", \"year_written\").filter(\"title LIKE 'Harry Potter'\").show()\n",
        "#if this dataset had many of the same books we could add a distinct() function at the end to only get 1 of each"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkAsUFrBBPuQ",
        "outputId": "adfcc005-e746-4585-c4f1-c7dcbc3540b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------+-----+------------+\n",
            "|       title|       edition|price|year_written|\n",
            "+------------+--------------+-----+------------+\n",
            "|Harry Potter|Harcourt Brace|19.95|        2000|\n",
            "+------------+--------------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using sparkSQL function we can get even more insight from our dataset, let's import it \n",
        "from pyspark.sql.functions import max"
      ],
      "metadata": {
        "id": "x9Lq-aulB7d5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's find the costliest books in our dataset\n",
        "#Get the max value for the price column\n",
        "maxValue = df.agg(max(\"price\")).collect()[0][0]\n",
        "print (maxValue)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoXjExARCfOn",
        "outputId": "866fea26-368b-4b2f-def8-a1502434c609"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We have the value, but what book or books have this price (could be many)\n",
        "df.select(\"title\",\"price\").filter(df.price == maxValue).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gec_ETJqDFrp",
        "outputId": "f1478800-6013-4b2d-d5b2-e3a0a5af8aec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+\n",
            "|              title|price|\n",
            "+-------------------+-----+\n",
            "|A Room of One's Own| 29.0|\n",
            "+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WXP5NgdeDo6S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}